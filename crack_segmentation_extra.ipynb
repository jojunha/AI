{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crack_Segmentation with UNet\n",
    "## dataset https://www.kaggle.com/datasets/lakshaymiddha/crack-segmentation-dataset?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_loading(train_path,image_size = 512):\n",
    "\n",
    "    out_rows = image_size\n",
    "    out_cols = image_size\n",
    "    \n",
    "    img_path = train_path + '/images'\n",
    "    mask_path = train_path + '/masks'\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    \n",
    "    imgs = glob.glob(img_path +\"/*.jpg\")\n",
    "    labels = glob.glob(mask_path +\"/*.jpg\")\n",
    "    imgdatas = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imglabels = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imgnames=[]\n",
    "    \n",
    "    for i, imgname in enumerate(imgs):\n",
    "        if i%100==0:\n",
    "            print('{}/{}'.format(i, len(imgs)))\n",
    "        name = os.path.split(imgname)[1][:-4]\n",
    "        img = load_img(imgname, color_mode = \"grayscale\")\n",
    "        labelname= mask_path + '/' + os.path.split(imgname)[1]\n",
    "        label = load_img(labelname, color_mode = \"grayscale\")\n",
    "        img=img.resize((out_rows,out_cols))\n",
    "        label=label.resize((out_rows,out_cols))\n",
    "\n",
    "        img = img_to_array(img)\n",
    "        label = img_to_array(label)\n",
    "        imgdatas[i] = img\n",
    "        imglabels[i] = label\n",
    "        imgnames.append(name)\n",
    "\n",
    "    imgdatas = imgdatas.astype('float32')\n",
    "    imglabels = imglabels.astype('float32')\n",
    "    \n",
    "    print('img : ', imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('normalization start...')\n",
    "    print('-'*30)\n",
    "    imgdatas = imgdatas/255.0\n",
    "    \n",
    "    imglabels[imglabels <= 127] = 0\n",
    "    imglabels[imglabels > 127] = 1\n",
    "    \n",
    "    print('img : ',imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    print('mask : ',imglabels.min())\n",
    "    print('loading done')\n",
    "    \n",
    "    return imgdatas, imglabels, imgnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkfolder(folder):\n",
    "    if not os.path.lexists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        return (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "        return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "def sens(y_true, y_pred): # sensitivity, recall\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    y_target_yn = K.round(K.clip(y_true, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "def sch(epoch):\n",
    "    if epoch>100 and epoch<=250:\n",
    "        return 0.0001\n",
    "    elif epoch>250:\n",
    "        return 0.00001\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(img_rows, img_cols):\n",
    "    inputs = Input((img_rows, img_cols,1))\n",
    "    conv1 = Conv2D(32, (3, 3), activation=None, padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation=None, padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation=None, padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation=None, padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation=None, padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation=None, padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation=None, padding='same')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation=None, padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation=None, padding='same')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation=None, padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=None, padding='same')(up6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=None, padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=None, padding='same')(up7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=None, padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=None, padding='same')(up8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=None, padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=None, padding='same')(up9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=None, padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CosineDecayRestarts Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmup(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epochs_per_cycle,iteration, max_lr, min_lr, verbose = 1):\n",
    "        self.epochs_per_cycle = epochs_per_cycle\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.iteration = iteration;\n",
    "        self.steps = 0;\n",
    "        self.learning_rate = max_lr;\n",
    "        self.epochs = 0; # epoch to search min_lr for each iteration \n",
    "        self.warmup_epoch = 10  # warmup epcch\n",
    "        self.verbose = verbose # log\n",
    "        self.lrates = list() # for graph\n",
    "        \n",
    "        \n",
    "    def cosine_annealing(self, epoch, epochs_per_cycle, max_lr):\n",
    "        self.epochs += 1; \n",
    "        cos_inner = (math.pi * (self.epochs % epochs_per_cycle)) / (epochs_per_cycle)\n",
    "        self.learning_rate = max_lr/2 * (math.cos(cos_inner) + 1)\n",
    "        \n",
    "        if ((self.epochs % epochs_per_cycle) == (epochs_per_cycle-1)):\n",
    "            self.steps += 1\n",
    "            self.max_lr *= 0.8\n",
    "            self.epochs = 0;\n",
    "            self.epochs_per_cycle = math.floor(self.epochs_per_cycle*1.2)\n",
    "            \n",
    "        return max_lr/2 * (math.cos(cos_inner) + 1)\n",
    "  \n",
    "    def warm_up(self, epoch):\n",
    "        \n",
    "        self.learning_rate = self.max_lr * epoch / self.warmup_epoch\n",
    "        \n",
    "        return self.learning_rate\n",
    "\n",
    "    # calculate and set learning rate at the start of the epoch\n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        if (epoch < self.warmup_epoch):\n",
    "            # warm up learning rate\n",
    "            lr = self.warm_up(epoch)\n",
    "       \n",
    "        elif(self.steps < self.iteration):\n",
    "            # calculate learning rate\n",
    "            lr = self.cosine_annealing(epoch, self.epochs_per_cycle, self.max_lr)\n",
    "            \n",
    "        else:\n",
    "            lr = self.min_lr\n",
    "        \n",
    "        if (self.verbose == 1):\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learng rate to %s.' % (epoch + 1, lr))  \n",
    "\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "#         self.lrates.append(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep(imgs_train, imgs_mask_train, path, batch_size = 4, epochs = 10, image_size=512, model = None): \n",
    "\n",
    "    if (model == None):\n",
    "        model = get_unet(image_size, image_size)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=0.001), loss=dice_coef_loss, \n",
    "                        metrics=['accuracy', sens, dice_coef_loss])\n",
    "    \n",
    "    check_model_path = path+'/crack_check'\n",
    "    predict_path = path+'/crack_pred'\n",
    "    mkfolder(check_model_path)\n",
    "    mkfolder(predict_path)\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(check_model_path + '/final_{epoch:d}_{loss:f}.hdf5', \n",
    "                                        monitor='val_dice_coef_loss',verbose=1, \n",
    "                                        save_best_only=False)\n",
    "#     earlystopping = EarlyStopping(monitor='val_dice_coef_loss', patience=30, restore_best_weights=True)\n",
    "#     cosine_schedule = CosineAnnealingWarmup(epochs_per_cycle=25, iteration=2,max_lr = 1e-3, min_lr = 1e-6)\n",
    "    \n",
    "    print('Fitting model...')\n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=batch_size, validation_split=0.1,epochs=epochs, verbose=1,shuffle=True, callbacks = [model_checkpoint])\n",
    "    print('save model')\n",
    "    model.save(predict_path + '/final.h5')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_save(pred_list,name_list,test_path):\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('Saving test images...')\n",
    "    print('-'*30)\n",
    "    \n",
    "    pred_img_path = test_path + '/pred'\n",
    "    mkfolder(pred_img_path)\n",
    "\n",
    "    imgs = pred_list\n",
    "    for i in range(imgs.shape[0]):\n",
    "        img = imgs[i]\n",
    "        img[img <= 0.5] = 0\n",
    "        img[img > 0.5] = 255\n",
    "        img = array_to_img(img)\n",
    "        img.save(pred_img_path+\"/%s_pred.jpg\" %(name_list[i]))\n",
    "\n",
    "def predict_val(model,test_path,image_size=512):\n",
    "    \n",
    "    imgs_test, imgs_label_test, test_name = create_test_data(test_path, image_size, image_size)\n",
    "    print('predict test data')\n",
    "    \n",
    "    imgs_label_pred = model.predict(imgs_test, batch_size=4, verbose=1)\n",
    "    name_list=test_name\n",
    "    df = pd.DataFrame(columns=['name', 'acc', 'sen', 'spe', 'dsc'],dtype = float)\n",
    "    df = df.astype({'name': 'str'})\n",
    "\n",
    "    true_list=imgs_label_test\n",
    "    print(true_list.shape)\n",
    "\n",
    "    pred_list=imgs_label_pred\n",
    "    print(np.unique(pred_list))\n",
    "    pred_list[pred_list > 0.5] = 1\n",
    "    pred_list[pred_list <= 0.5] = 0\n",
    "    \n",
    "#     sensitivity=[]\n",
    "#     specificity=[]\n",
    "#     acc=[]\n",
    "#     dsc=[]\n",
    "\n",
    "#     for i in range(len(true_list)):\n",
    "#         yt=true_list[i].flatten()\n",
    "#         yp=pred_list[i].flatten()\n",
    "#         mat=confusion_matrix(yt,yp)\n",
    "#         if len(mat) == 2:\n",
    "#             ac=(mat[1,1]+mat[0,0])/(mat[1,0]+mat[1,1]+mat[0,1]+mat[0,0])\n",
    "#             st=mat[1,1]/(mat[1,0]+mat[1,1])\n",
    "#             sp=mat[0,0]/(mat[0,1]+mat[0,0])\n",
    "#             if mat[1,0]+mat[1,1] == 0:\n",
    "#                 specificity.append(sp)\n",
    "#                 acc.append(ac)\n",
    "#             else:\n",
    "#                 sensitivity.append(st)  \n",
    "#                 specificity.append(sp)\n",
    "#                 acc.append(ac)\n",
    "#         else:\n",
    "#             specificity.append(1)\n",
    "#             acc.append(1)\n",
    "\n",
    "#         yt=true_list[i]\n",
    "#         yp=pred_list[i]\n",
    "#         if np.sum(yt) != 0 and np.sum(yp) != 0:\n",
    "#             dice = np.sum(yp[yt==1])*2.0 / (np.sum(yt) + np.sum(yp))\n",
    "#             dsc.append(dice)\n",
    "#             df=  df.append({'name':name_list[i], 'acc':ac, 'sen':st, 'spe':sp, 'dsc':dice}, ignore_index=True)\n",
    "\n",
    "#     print(\"complete\")      \n",
    "#     print(\"acc avg : {0:0.4f}\".format(np.mean(acc)))\n",
    "#     print(\"sensitivity avg : {0:0.4f}\".format(np.mean(sensitivity)))\n",
    "#     print(\"specificity avg : {0:0.4f}\".format(np.mean(specificity)))\n",
    "#     print(\"dsc avg : {0:0.4f}\".format(np.mean(dsc)))\n",
    "    \n",
    "    \n",
    "\n",
    "    predict_save(pred_list,name_list,test_path)\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('!! Finish Test !!')\n",
    "    return test_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data(test_path, out_rows, out_cols):\n",
    "    \n",
    "    img_path = test_path + '/images'\n",
    "    mask_path = test_path + '/masks'\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('Creating test images...')\n",
    "    print('-'*30)\n",
    "    \n",
    "    imgs = glob.glob(img_path +\"/*.jpg\")\n",
    "    labels = glob.glob(mask_path +\"/*.jpg\")\n",
    "    imgdatas = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imglabels = np.ndarray((len(imgs),out_rows,out_cols,1), dtype=np.uint8)\n",
    "    imgnames=[]\n",
    "    \n",
    "    for i, imgname in enumerate(imgs):\n",
    "        if i%100==0:\n",
    "            print('{}/{}'.format(i, len(imgs)))\n",
    "        name = os.path.split(imgname)[1][:-4]\n",
    "        img = load_img(imgname, color_mode = \"grayscale\")\n",
    "        labelname= mask_path + '/' + os.path.split(imgname)[1]\n",
    "        label = load_img(labelname, color_mode = \"grayscale\")\n",
    "        img=img.resize((out_rows,out_cols))\n",
    "        label=label.resize((out_rows,out_cols))\n",
    "\n",
    "        img = img_to_array(img)\n",
    "        label = img_to_array(label)\n",
    "        imgdatas[i] = img\n",
    "        imglabels[i] = label\n",
    "        imgnames.append(name)\n",
    "\n",
    "    imgdatas = imgdatas.astype('float32')\n",
    "    imglabels = imglabels.astype('float32')\n",
    "    \n",
    "    print('img : ', imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('normalization start...')\n",
    "    print('-'*30)\n",
    "    imgdatas = imgdatas/255.0\n",
    "    \n",
    "    imglabels[imglabels <= 127] = 0\n",
    "    imglabels[imglabels > 127] = 1\n",
    "    \n",
    "    print('img : ',imgdatas.max())\n",
    "    print('mask : ',imglabels.max())\n",
    "    print('mask : ',imglabels.min())\n",
    "    print('loading done')\n",
    "    \n",
    "    return imgdatas, imglabels, imgnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "0/9603\n",
      "100/9603\n",
      "200/9603\n",
      "300/9603\n",
      "400/9603\n",
      "500/9603\n",
      "600/9603\n",
      "700/9603\n",
      "800/9603\n",
      "900/9603\n",
      "1000/9603\n",
      "1100/9603\n",
      "1200/9603\n",
      "1300/9603\n",
      "1400/9603\n",
      "1500/9603\n",
      "1600/9603\n",
      "1700/9603\n",
      "1800/9603\n",
      "1900/9603\n",
      "2000/9603\n",
      "2100/9603\n",
      "2200/9603\n",
      "2300/9603\n",
      "2400/9603\n",
      "2500/9603\n",
      "2600/9603\n",
      "2700/9603\n",
      "2800/9603\n",
      "2900/9603\n",
      "3000/9603\n",
      "3100/9603\n",
      "3200/9603\n",
      "3300/9603\n",
      "3400/9603\n",
      "3500/9603\n",
      "3600/9603\n",
      "3700/9603\n",
      "3800/9603\n",
      "3900/9603\n",
      "4000/9603\n",
      "4100/9603\n",
      "4200/9603\n",
      "4300/9603\n",
      "4400/9603\n",
      "4500/9603\n",
      "4600/9603\n",
      "4700/9603\n",
      "4800/9603\n",
      "4900/9603\n",
      "5000/9603\n",
      "5100/9603\n",
      "5200/9603\n",
      "5300/9603\n",
      "5400/9603\n",
      "5500/9603\n",
      "5600/9603\n",
      "5700/9603\n",
      "5800/9603\n",
      "5900/9603\n",
      "6000/9603\n",
      "6100/9603\n",
      "6200/9603\n",
      "6300/9603\n",
      "6400/9603\n",
      "6500/9603\n",
      "6600/9603\n",
      "6700/9603\n",
      "6800/9603\n",
      "6900/9603\n",
      "7000/9603\n",
      "7100/9603\n",
      "7200/9603\n",
      "7300/9603\n",
      "7400/9603\n",
      "7500/9603\n",
      "7600/9603\n",
      "7700/9603\n",
      "7800/9603\n",
      "7900/9603\n",
      "8000/9603\n",
      "8100/9603\n",
      "8200/9603\n",
      "8300/9603\n",
      "8400/9603\n",
      "8500/9603\n",
      "8600/9603\n",
      "8700/9603\n",
      "8800/9603\n",
      "8900/9603\n",
      "9000/9603\n",
      "9100/9603\n",
      "9200/9603\n",
      "9300/9603\n",
      "9400/9603\n",
      "9500/9603\n",
      "9600/9603\n",
      "img :  255.0\n",
      "mask :  255.0\n",
      "------------------------------\n",
      "normalization start...\n",
      "------------------------------\n",
      "img :  1.0\n",
      "mask :  1.0\n",
      "mask :  0.0\n",
      "loading done\n",
      "Fitting model...\n",
      "Epoch 1/20\n",
      "Tensor(\"model/conv2d_18/Sigmoid:0\", shape=(None, 512, 512, 1), dtype=float32)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(None, 512, 512, 1), dtype=float32)\n",
      "Tensor(\"model/conv2d_18/Sigmoid:0\", shape=(None, 512, 512, 1), dtype=float32)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(None, 512, 512, 1), dtype=float32)\n",
      "2161/2161 [==============================] - ETA: 0s - loss: 0.1530 - accuracy: 0.9914 - sens: 0.8355 - dice_coef_loss: 0.1530Tensor(\"model/conv2d_18/Sigmoid:0\", shape=(None, 512, 512, 1), dtype=float32)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(None, 512, 512, 1), dtype=float32)\n",
      "2161/2161 [==============================] - 783s 354ms/step - loss: 0.1530 - accuracy: 0.9914 - sens: 0.8355 - dice_coef_loss: 0.1530 - val_loss: 0.4586 - val_accuracy: 0.9682 - val_sens: 0.4007 - val_dice_coef_loss: 0.4583\n",
      "\n",
      "Epoch 00001: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_1_0.156544.hdf5\n",
      "Epoch 2/20\n",
      "2161/2161 [==============================] - 757s 351ms/step - loss: 0.1565 - accuracy: 0.9914 - sens: 0.8318 - dice_coef_loss: 0.1565 - val_loss: 0.4808 - val_accuracy: 0.9673 - val_sens: 0.3779 - val_dice_coef_loss: 0.4806\n",
      "\n",
      "Epoch 00002: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_2_0.156115.hdf5\n",
      "Epoch 3/20\n",
      "2161/2161 [==============================] - 757s 350ms/step - loss: 0.1546 - accuracy: 0.9914 - sens: 0.8333 - dice_coef_loss: 0.1546 - val_loss: 0.4798 - val_accuracy: 0.9676 - val_sens: 0.3780 - val_dice_coef_loss: 0.4795\n",
      "\n",
      "Epoch 00003: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_3_0.156288.hdf5\n",
      "Epoch 4/20\n",
      "2161/2161 [==============================] - 758s 351ms/step - loss: 0.1531 - accuracy: 0.9915 - sens: 0.8340 - dice_coef_loss: 0.1531 - val_loss: 0.4369 - val_accuracy: 0.9687 - val_sens: 0.4277 - val_dice_coef_loss: 0.4368\n",
      "\n",
      "Epoch 00004: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_4_0.155057.hdf5\n",
      "Epoch 5/20\n",
      "2161/2161 [==============================] - 760s 352ms/step - loss: 0.1557 - accuracy: 0.9914 - sens: 0.8336 - dice_coef_loss: 0.1557 - val_loss: 0.4894 - val_accuracy: 0.9669 - val_sens: 0.3694 - val_dice_coef_loss: 0.4891\n",
      "\n",
      "Epoch 00005: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_5_0.155295.hdf5\n",
      "Epoch 6/20\n",
      "2161/2161 [==============================] - 758s 351ms/step - loss: 0.1557 - accuracy: 0.9914 - sens: 0.8312 - dice_coef_loss: 0.1557 - val_loss: 0.5060 - val_accuracy: 0.9665 - val_sens: 0.3519 - val_dice_coef_loss: 0.5057\n",
      "\n",
      "Epoch 00006: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_6_0.152647.hdf5\n",
      "Epoch 7/20\n",
      "2161/2161 [==============================] - 757s 350ms/step - loss: 0.1545 - accuracy: 0.9913 - sens: 0.8324 - dice_coef_loss: 0.1545 - val_loss: 0.4861 - val_accuracy: 0.9672 - val_sens: 0.3720 - val_dice_coef_loss: 0.4859\n",
      "\n",
      "Epoch 00007: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_7_0.153420.hdf5\n",
      "Epoch 8/20\n",
      "2161/2161 [==============================] - 757s 350ms/step - loss: 0.1544 - accuracy: 0.9913 - sens: 0.8319 - dice_coef_loss: 0.1544 - val_loss: 0.4738 - val_accuracy: 0.9673 - val_sens: 0.3868 - val_dice_coef_loss: 0.4737\n",
      "\n",
      "Epoch 00008: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_8_0.153738.hdf5\n",
      "Epoch 9/20\n",
      "2161/2161 [==============================] - 755s 350ms/step - loss: 0.1553 - accuracy: 0.9915 - sens: 0.8323 - dice_coef_loss: 0.1553 - val_loss: 0.4565 - val_accuracy: 0.9681 - val_sens: 0.4040 - val_dice_coef_loss: 0.4563\n",
      "\n",
      "Epoch 00009: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_9_0.153966.hdf5\n",
      "Epoch 10/20\n",
      "2161/2161 [==============================] - 760s 352ms/step - loss: 0.1532 - accuracy: 0.9916 - sens: 0.8340 - dice_coef_loss: 0.1532 - val_loss: 0.4702 - val_accuracy: 0.9678 - val_sens: 0.3880 - val_dice_coef_loss: 0.4700\n",
      "\n",
      "Epoch 00010: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_10_0.152611.hdf5\n",
      "Epoch 11/20\n",
      "2161/2161 [==============================] - 758s 351ms/step - loss: 0.1530 - accuracy: 0.9915 - sens: 0.8345 - dice_coef_loss: 0.1530 - val_loss: 0.4726 - val_accuracy: 0.9675 - val_sens: 0.3873 - val_dice_coef_loss: 0.4723\n",
      "\n",
      "Epoch 00011: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_11_0.152956.hdf5\n",
      "Epoch 12/20\n",
      "2161/2161 [==============================] - 755s 349ms/step - loss: 0.1506 - accuracy: 0.9915 - sens: 0.8359 - dice_coef_loss: 0.1506 - val_loss: 0.4853 - val_accuracy: 0.9671 - val_sens: 0.3728 - val_dice_coef_loss: 0.4850\n",
      "\n",
      "Epoch 00012: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_12_0.152036.hdf5\n",
      "Epoch 13/20\n",
      "2161/2161 [==============================] - 758s 351ms/step - loss: 0.1482 - accuracy: 0.9918 - sens: 0.8389 - dice_coef_loss: 0.1482 - val_loss: 0.4624 - val_accuracy: 0.9678 - val_sens: 0.3985 - val_dice_coef_loss: 0.4622\n",
      "\n",
      "Epoch 00013: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_13_0.151633.hdf5\n",
      "Epoch 14/20\n",
      "2161/2161 [==============================] - 757s 350ms/step - loss: 0.1499 - accuracy: 0.9917 - sens: 0.8368 - dice_coef_loss: 0.1499 - val_loss: 0.5047 - val_accuracy: 0.9661 - val_sens: 0.3551 - val_dice_coef_loss: 0.5045\n",
      "\n",
      "Epoch 00014: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_14_0.152735.hdf5\n",
      "Epoch 15/20\n",
      "2161/2161 [==============================] - 756s 350ms/step - loss: 0.1499 - accuracy: 0.9916 - sens: 0.8361 - dice_coef_loss: 0.1499 - val_loss: 0.4887 - val_accuracy: 0.9671 - val_sens: 0.3689 - val_dice_coef_loss: 0.4883\n",
      "\n",
      "Epoch 00015: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_15_0.150833.hdf5\n",
      "Epoch 16/20\n",
      "2161/2161 [==============================] - 756s 350ms/step - loss: 0.1502 - accuracy: 0.9917 - sens: 0.8353 - dice_coef_loss: 0.1502 - val_loss: 0.4640 - val_accuracy: 0.9677 - val_sens: 0.3973 - val_dice_coef_loss: 0.4637\n",
      "\n",
      "Epoch 00016: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_16_0.151958.hdf5\n",
      "Epoch 17/20\n",
      "2161/2161 [==============================] - 757s 350ms/step - loss: 0.1507 - accuracy: 0.9916 - sens: 0.8350 - dice_coef_loss: 0.1507 - val_loss: 0.4888 - val_accuracy: 0.9670 - val_sens: 0.3688 - val_dice_coef_loss: 0.4886\n",
      "\n",
      "Epoch 00017: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_17_0.150696.hdf5\n",
      "Epoch 18/20\n",
      "2161/2161 [==============================] - 757s 350ms/step - loss: 0.1526 - accuracy: 0.9916 - sens: 0.8332 - dice_coef_loss: 0.1526 - val_loss: 0.4573 - val_accuracy: 0.9679 - val_sens: 0.4043 - val_dice_coef_loss: 0.4571\n",
      "\n",
      "Epoch 00018: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_18_0.150126.hdf5\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2161/2161 [==============================] - 759s 351ms/step - loss: 0.1543 - accuracy: 0.9915 - sens: 0.8314 - dice_coef_loss: 0.1543 - val_loss: 0.4570 - val_accuracy: 0.9682 - val_sens: 0.4028 - val_dice_coef_loss: 0.4569\n",
      "\n",
      "Epoch 00019: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_19_0.152934.hdf5\n",
      "Epoch 20/20\n",
      "2161/2161 [==============================] - 760s 352ms/step - loss: 0.1515 - accuracy: 0.9917 - sens: 0.8341 - dice_coef_loss: 0.1515 - val_loss: 0.4792 - val_accuracy: 0.9673 - val_sens: 0.3794 - val_dice_coef_loss: 0.4790\n",
      "\n",
      "Epoch 00020: saving model to C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check\\final_20_0.150461.hdf5\n",
      "save model\n",
      "------------------------------\n",
      "Creating test images...\n",
      "------------------------------\n",
      "0/1695\n",
      "100/1695\n",
      "200/1695\n",
      "300/1695\n",
      "400/1695\n",
      "500/1695\n",
      "600/1695\n",
      "700/1695\n",
      "800/1695\n",
      "900/1695\n",
      "1000/1695\n",
      "1100/1695\n",
      "1200/1695\n",
      "1300/1695\n",
      "1400/1695\n",
      "1500/1695\n",
      "1600/1695\n",
      "img :  255.0\n",
      "mask :  255.0\n",
      "------------------------------\n",
      "normalization start...\n",
      "------------------------------\n",
      "img :  1.0\n",
      "mask :  1.0\n",
      "mask :  0.0\n",
      "loading done\n",
      "predict test data\n",
      "424/424 [==============================] - 48s 110ms/step\n",
      "(1695, 512, 512, 1)\n",
      "[0.0000000e+00 1.1754997e-38 1.1755086e-38 ... 9.9999976e-01 9.9999988e-01\n",
      " 1.0000000e+00]\n",
      "------------------------------\n",
      "Saving test images...\n",
      "------------------------------\n",
      "------------------------------\n",
      "!! Finish Test !!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 주어진 Train Dataset 경로.\n",
    "    path = 'C:/Users/s_wnsgk4041/crack_segmentation_dataset'\n",
    "    train_path = 'C:/Users/s_wnsgk4041/crack_segmentation_dataset/train'\n",
    "    test_path = 'C:/Users/s_wnsgk4041/crack_segmentation_dataset/test'\n",
    "    image_size = 512\n",
    "    epochs = 20\n",
    "    batch_sizes = 4\n",
    "    \n",
    "    imgs_train, imgs_mask_train, imgs_name = train_data_loading(train_path, image_size = image_size)\n",
    "    \n",
    "    model = load_model('C:/Users/s_wnsgk4041/crack_segmentation_dataset/crack_check/final_49_0.154663.hdf5',\n",
    "                                       custom_objects={'sens':sens,'dice_coef_loss': dice_coef_loss})\n",
    "    \n",
    "    model = deep(imgs_train, imgs_mask_train, path, batch_size = batch_sizes, epochs = epochs, image_size=image_size, model=model)\n",
    "\n",
    "    test_name = predict_val(model, test_path, image_size = image_size)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
